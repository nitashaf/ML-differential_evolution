{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbe65057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29badcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this class is merely for encapsulation.\n",
    "#this is how data of the dataset is stores in the objects \n",
    "#of this class.\n",
    "class DataSet:\n",
    "    #name of the dataset\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    \n",
    "    def set_feature(self, features):\n",
    "        self.features = features\n",
    "    \n",
    "    def set_classes(self, classes):\n",
    "        self.classes = classes\n",
    "    \n",
    "    def get_classes(self):\n",
    "        return self.classes\n",
    "        \n",
    "    def set_org_data(self, org_data):\n",
    "        self.org_data = org_data\n",
    "        \n",
    "    def get_org_data(self):\n",
    "        return self.org_data\n",
    "    \n",
    "    # not writing all getters, setters, because\n",
    "    # they are not required in python (i didnt know that before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c61a218",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class reads the data file and the names file\n",
    "#to load inti dataset object\n",
    "class Dataset_Reader:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.Path = 'C:\\\\Users\\\\nitas\\\\Downloads\\\\Machine Learning 2024\\\\MLProject2\\\\'\n",
    "    \n",
    "    #read the data from data file and fill the Datset class object\n",
    "    def read_data_file(self, ds):\n",
    "        if(ds.name == 'forestfires'):\n",
    "            df = pd.read_csv(self.Path+ds.name+'.data', delimiter=',')\n",
    "        else:    \n",
    "            df = pd.read_csv(self.Path+ds.name+'.data', delimiter=',', header=None)\n",
    "\n",
    "        ds.set_org_data(df)\n",
    "    \n",
    "    \n",
    "    #read name file, naming the features, just to fill\n",
    "    #for results, not mendatory function\n",
    "    def read_attributes(self, ds):\n",
    "        \n",
    "        start_string = 'Attribute Information'\n",
    "        stop_string = 'Missing Attribute Values'\n",
    "        isPrint = False\n",
    "        \n",
    "        #read section, where the file has information \n",
    "        #Attribute Information\n",
    "        with open(Path+ds.name+'.names', 'r') as file:\n",
    "            for line in file:\n",
    "                if start_string in line:\n",
    "                    isPrint = True\n",
    "                    #continue\n",
    "                    \n",
    "                if stop_string in line:\n",
    "                    isPrint = False\n",
    "                    break\n",
    "                        \n",
    "                if isPrint:\n",
    "                    print(line.strip())\n",
    "                        "
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea596c82",
   "metadata": {},
   "source": [
    "# Pre Processing Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12364246",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre Process Class, for data cleaning, \n",
    "#checking for any abnormalities, pre processes the data \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "class PreProcess:\n",
    "    \n",
    "    def __init__(self, ds):\n",
    "        \n",
    "        #set other values of the Dataset object    \n",
    "        self.set_Dataset_features(ds)\n",
    "        \n",
    "        #min max scaling for the regression datasets\n",
    "        if(ds.type == 'Regression'):\n",
    "            self.min_max_scaling(ds)\n",
    "        if(ds.type == 'Classification'):\n",
    "            self.min_max_scaling_class(ds)\n",
    "        #now first step is to do one-hot encoding\n",
    "        self.one_hot_encoding(ds)\n",
    "        #Now sort the data, if it is not sorted\n",
    "        self.sort_data(ds)\n",
    "        #After this step, we will use only sorted data\n",
    "        self.missing_values(ds)\n",
    "    \n",
    "    \n",
    "    #Zscore Normalization \n",
    "    def zscore_normalization(self, ds):\n",
    "        scaler = StandardScaler()\n",
    "        numeric_cols = ds.org_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "        ds.org_data[numeric_cols] = scaler.fit_transform(ds.org_data[numeric_cols])\n",
    "    \n",
    "    \n",
    "    def zscore_normalization_class(self, ds):\n",
    "        scaler = StandardScaler()\n",
    "        numeric_cols = ds.org_data.iloc[:, :-1].select_dtypes(include=['float64', 'int64']).columns\n",
    "        ds.org_data[numeric_cols] = scaler.fit_transform(ds.org_data[numeric_cols])\n",
    "        \n",
    "    #Min max scaling \n",
    "    def min_max_scaling(self, ds):\n",
    "        scaler = MinMaxScaler()\n",
    "        #print(ds.org_data.head())\n",
    "        # Apply scaling only to numeric columns\n",
    "        numeric_cols = ds.org_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "        #print(numeric_cols)\n",
    "        ds.org_data[numeric_cols] = scaler.fit_transform(ds.org_data[numeric_cols]) \n",
    "        #print('After applying Min Max Scaling:' ,ds.org_data.head())\n",
    "        #Min max scaling \n",
    "        \n",
    "    #for classification, we will exclude the class column    \n",
    "    def min_max_scaling_class(self, ds):\n",
    "        scaler = MinMaxScaler()\n",
    "        #print(ds.org_data.head())\n",
    "        # Apply scaling only to numeric columns\n",
    "        numeric_cols = ds.org_data.iloc[:, :-1].select_dtypes(include=['float64', 'int64']).columns\n",
    "        #print(numeric_cols)\n",
    "        ds.org_data[numeric_cols] = scaler.fit_transform(ds.org_data[numeric_cols]) \n",
    "        #print('After applying Min Max Scaling:' ,ds.org_data.head())\n",
    "    \n",
    "    #This function sets the no of features and class names\n",
    "    #to the Dataset class object\n",
    "    def set_Dataset_features(self, ds):\n",
    "        #set class names\n",
    "        last_column_unique = ds.org_data.iloc[:, -1].unique()\n",
    "        ds.set_classes(last_column_unique)\n",
    "        #print(ds.get_classes())\n",
    "        \n",
    "        #set no of features, because one column is classes\n",
    "        ds.feature_count = ds.org_data.shape[1] -1\n",
    "        #print(ds.feature_count)\n",
    "    \n",
    "                \n",
    "    #splitting data into 10 parts, Here we will\n",
    "    #implement stratified 10 folds\n",
    "    def split_data_regression(self,ds):\n",
    "        \n",
    "        # first we need to make 10 datasets of consectutive points\n",
    "        #print('\\n Printing the results of 10 folds')\n",
    "        datagroupSize = len(ds.sorted_data) // 10  \n",
    "        #print('Fold Size', datagroupSize)\n",
    "        dataGroup = []\n",
    "        \n",
    "        #reusing this part from my first project\n",
    "        for i in range(10):\n",
    "            start = i * datagroupSize\n",
    "            end = start + datagroupSize\n",
    "            if i == 9:  \n",
    "                dataGroup.append(ds.sorted_data.iloc[start:])\n",
    "            else:\n",
    "                dataGroup.append(ds.sorted_data.iloc[start:end])\n",
    "        \n",
    "        \n",
    "        #once we have datasets of sorted data, now we will make folds\n",
    "        #with one point from each fold.\n",
    "        folds = [[] for _ in range(10)]\n",
    "        \n",
    "        max_group_size = max(len(group) for group in dataGroup)\n",
    "        #print('Max Fold Size: ',max_group_size)\n",
    "        \n",
    "        # For each index up to the maximum group size\n",
    "        for i in range(max_group_size):\n",
    "            # Loop over the 10 groups\n",
    "            for j in range(10):\n",
    "                #print('fold:', j, 'datagroup :' ,j, 'value',  dataGroup[j].iloc[i, -1])\n",
    "                # Check if the current group has enough data for the current index\n",
    "                if i < len(dataGroup[j]):  \n",
    "                    # Append the i-th example from the j-th group into the j-th fold\n",
    "                    folds[j].append(dataGroup[j].iloc[i])\n",
    "\n",
    "                    \n",
    "        #converting folds into Dataframes\n",
    "        final_folds = folds = [pd.DataFrame(fold) for fold in folds]\n",
    "        ds.ten_folds = final_folds\n",
    "        \n",
    "        #for i in range(10):\n",
    "        #    print(f'Fold {i + 1}:')\n",
    "        #    print(final_folds[i].tail())\n",
    "        \n",
    "        #print(\"Ten Fold Prints\")\n",
    "        #for i, fold in enumerate(final_folds):\n",
    "        #    print(f'Fold {i+1} size: {len(fold)}')\n",
    "        #    print(fold.iloc[:, -1].value_counts())\n",
    "    \n",
    "    \n",
    "    #splitting data into 10 parts, Here we will\n",
    "    #implement stratified 10 folds for classification\n",
    "    def split_data_classification(self,ds):\n",
    "        \n",
    "        #In this we first need to split data accordign to classes\n",
    "        #Since we want the ratio of classes to be same in each fold\n",
    "        classwise_data = {}\n",
    "        \n",
    "        for cls in ds.classes:\n",
    "            classwise_data[cls] = ds.sorted_data[ds.sorted_data.iloc[:, -1] == cls]\n",
    "                    \n",
    "        # Now we will be creating the folds containing data from \n",
    "        # each fold of class, making the ratio same with original data\n",
    "        folds = [[] for _ in range(10)]\n",
    "        \n",
    "        for cls, cls_data in classwise_data.items():\n",
    "            \n",
    "            #first divide the classwise data into 10 parts.\n",
    "            clsPartsforFolds = len(cls_data) //10\n",
    "            clsPartsRemain = len(cls_data) % 10\n",
    "            \n",
    "            #print('Number of class data per fold',clsPartsforFolds)\n",
    "            #print('Remaining data left in class',clsPartsRemain)\n",
    "            #put these values in all folds\n",
    "            start = 0\n",
    "            for i in range(10):\n",
    "                if clsPartsRemain != 0:\n",
    "                    # Add 1 to handle remaining data\n",
    "                    end = start + clsPartsforFolds + 1  \n",
    "                    clsPartsRemain -= 1\n",
    "                else:\n",
    "                    end = start + clsPartsforFolds\n",
    "            \n",
    "                # Append class data slice to the respective fold\n",
    "                folds[i].extend(cls_data.iloc[start:end].values.tolist())\n",
    "                start = end\n",
    "            \n",
    "        \n",
    "        #converting folds into Dataframes\n",
    "        final_folds = [pd.DataFrame(fold, columns=ds.sorted_data.columns) for fold in folds]    \n",
    "        ds.ten_folds = final_folds\n",
    "        \n",
    "        #This section is just to check if the fold created are in the same ratio\n",
    "        #for i in range(10):\n",
    "        #    print(f'Fold {i + 1}:')\n",
    "        #    print(final_folds[i].tail())\n",
    "        \n",
    "        #for i, fold in enumerate(final_folds):\n",
    "        #    print(f'Fold {i+1} size: {len(fold)}')\n",
    "        #    print(fold.iloc[:, -1].value_counts())\n",
    "        \n",
    "    \n",
    "    #added the function to see, if we will need it for any dataset\n",
    "    def one_hot_encoding(self, ds):\n",
    "        #this has first coloumn of categorical data, F, M and I\n",
    "        if(ds.name == 'abalone'):\n",
    "            #save class column and drop it\n",
    "            class_column = ds.org_data.iloc[:, -1]  \n",
    "            ds.org_data = ds.org_data.drop(ds.org_data.columns[-1], axis=1)\n",
    "            \n",
    "            #perform one hot encoding    \n",
    "            first_column_name = ds.org_data.columns[0]\n",
    "            one_hot_encoded = pd.get_dummies(ds.org_data[first_column_name], prefix=first_column_name)\n",
    "            ds.org_data = pd.concat([ds.org_data.drop(columns=[first_column_name]), one_hot_encoded], axis=1)\n",
    "            #append the saved class column to the end again\n",
    "            ds.org_data = pd.concat([ds.org_data, class_column], axis=1)\n",
    "        \n",
    "        #this dataset has first column of manufacturer, and second column of \n",
    "        #model name, which has unique values, so dropping it.\n",
    "        if(ds.name == 'machine'):\n",
    "            \n",
    "            #here i am dropping the last colum, because class / result is \n",
    "            ds.org_data = ds.org_data.drop(ds.org_data.columns[-1], axis=1)\n",
    "            #save the class column and then drop it   \n",
    "            class_column = ds.org_data.iloc[:, -1]\n",
    "            ds.org_data = ds.org_data.drop(ds.org_data.columns[-1], axis=1)\n",
    "            \n",
    "            #drop the second column, which is model number\n",
    "            ds.org_data = ds.org_data.drop(ds.org_data.columns[1], axis=1)\n",
    "            first_column_name = ds.org_data.columns[0]\n",
    "            one_hot_encoded = pd.get_dummies(ds.org_data[first_column_name], prefix=first_column_name)\n",
    "            ds.org_data = pd.concat([ds.org_data.drop(columns=[first_column_name]), one_hot_encoded], axis=1)\n",
    "            \n",
    "            #append the saved class column to the end again\n",
    "            ds.org_data = pd.concat([ds.org_data, class_column], axis=1)\n",
    "        \n",
    "        #first column is months, second column is days\n",
    "        if ds.name == 'forestfires':\n",
    "            \n",
    "            #tranform the last column to log \n",
    "            #*****This is still not helping, we might need normalization*****\n",
    "            ds.org_data['area'] = np.log(ds.org_data['area'] +  1)\n",
    "            \n",
    "            \n",
    "            #save the class column and then drop it, \n",
    "            class_column = ds.org_data.iloc[:, -1]\n",
    "            ds.org_data = ds.org_data.drop(ds.org_data.columns[-1], axis=1)\n",
    "            \n",
    "            first_column_name = ds.org_data.columns[2]\n",
    "            second_column_name = ds.org_data.columns[3]\n",
    "            one_hot_encoded = pd.get_dummies(ds.org_data[[first_column_name, second_column_name]], \n",
    "                                             prefix=[first_column_name, second_column_name])\n",
    "                                    \n",
    "            ds.org_data = pd.concat([ds.org_data.drop(columns=[first_column_name, second_column_name]), \n",
    "                                     one_hot_encoded], axis=1)\n",
    "            \n",
    "            #append the saved class column to the end again\n",
    "            ds.org_data = pd.concat([ds.org_data, class_column], axis=1)\n",
    "        \n",
    "        #print(\"One hot Encoding Step, Resulted data shown below\")\n",
    "        #print(ds.org_data.head())\n",
    "        \n",
    "    #First we need to sort the data, if not sorted according to the \n",
    "    #classes, if data is already sorted, then original values will remain\n",
    "    #same without change\n",
    "    def sort_data(self, ds):\n",
    "        ds.sorted_data = ds.org_data.sort_values(by=ds.org_data.columns[-1])\n",
    "        #and then reset the index\n",
    "        ds.sorted_data = ds.sorted_data.reset_index(drop=True)\n",
    "        \n",
    "        \n",
    "    # this function handles missing values, for this project\n",
    "    # forest fires has no missing values\n",
    "    # Abalone has no missing values\n",
    "    # machine has no missing values\n",
    "    # glass has no missing values\n",
    "    # Soybean has no missing values\n",
    "    # breastCancer has missing values\n",
    "    def missing_values(self, ds):      \n",
    "        # Check if any missing values exist, using '?' and null values\n",
    "        ds.sorted_data.replace('?', np.nan, inplace=True)\n",
    "        has_missing_values = ds.sorted_data.isnull().values.any()\n",
    "        missing_values_count = ds.sorted_data.isnull().sum().sum()\n",
    "        \n",
    "        if (has_missing_values):\n",
    "            #print(\"This dataset has missing values\")\n",
    "            if missing_values_count > 1:\n",
    "                # It has missing values  \n",
    "                #print(\"This dataset has more missing values\")\n",
    "                rows_with_missing = dds.sorted_data[ds.sorted_data.isnull().any(axis=1)].index\n",
    "                #print(\"Rows with missing values before filling:\")\n",
    "                #print(ds.ds.sorted_data.loc[rows_with_missing])\n",
    "                \n",
    "                if(ds.name == 'breast-cancer-wisconsin'):\n",
    "                    #print(\"Inside the median if\")\n",
    "                    for column in ds.sorted_data.columns[:-1]:\n",
    "                        median_value = ds.sorted_data[column].median()\n",
    "                        # Filling the missing values\n",
    "                        ds.sorted_data[column].fillna(median_value, inplace=True)\n",
    "    \n",
    "                #print(\"Rows with missing values after filling:\")\n",
    "                #print(ds.sorted_data.loc[rows_with_missing])\n",
    "            else:\n",
    "                ds.sorted_data.dropna(inplace=True)\n",
    "                print(\"Dropped the row\")\n",
    "\n",
    "    #This one function will work for classification    \n",
    "    def split_tuning_data_classification(self, ds):\n",
    "        #print('Spliting the data into tunning and remaining')\n",
    "        # Backup sorted data\n",
    "        ds.sorted_data_bk = ds.sorted_data.copy()\n",
    "    \n",
    "        # Initialize empty lists for tuning and remaining data\n",
    "        tuning_data = []\n",
    "        remaining_data = []\n",
    "\n",
    "        # Separate the data into classes\n",
    "        for cls in ds.classes:\n",
    "            #print(cls)\n",
    "            classwise_data = ds.sorted_data[ds.sorted_data.iloc[:, -1] == cls]\n",
    "\n",
    "            # Shuffle the data\n",
    "            classwise_data = classwise_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        \n",
    "            # Extract 10% tuning set\n",
    "            num_tuning_samples = max(1, int(len(classwise_data) * 0.1))  \n",
    "            #print('number of samples from this class',num_tuning_samples )\n",
    "        \n",
    "            # Append tuning set and remaining data\n",
    "            tuning_data.append(classwise_data.iloc[:num_tuning_samples])\n",
    "            remaining_data.append(classwise_data.iloc[num_tuning_samples:])\n",
    "    \n",
    "        # Concatenate tuning and remaining data\n",
    "        ds.tuning_data = pd.concat(tuning_data).reset_index(drop=True)\n",
    "        ds.sorted_data = pd.concat(remaining_data).reset_index(drop=True)\n",
    "        \n",
    "        #print('Length of tuning data:',len(ds.tuning_data) )\n",
    "        #print('Length of remaining data:',len(ds.sorted_data) )\n",
    "        #print('Length of original data:',len(ds.sorted_data_bk) )\n",
    "\n",
    "        \n",
    "    def split_tuning_data_regression(self, ds):\n",
    "        # Backup sorted data\n",
    "        \n",
    "        #print('\\n Spliting the data into tunning and remaining')\n",
    "        tuning_fraction = 0.1\n",
    "        n_bins = 10\n",
    "        ds.sorted_data_bk = ds.sorted_data.copy()\n",
    "\n",
    "        # Bin the target variable into quantiles (or any number of bins)\n",
    "        ds.sorted_data['bins'] = pd.qcut(ds.sorted_data.iloc[:, -1], q=n_bins, labels=False, duplicates='drop')\n",
    "\n",
    "        # Initialize empty lists for tuning and remaining data\n",
    "        tuning_data = []\n",
    "        remaining_data = []\n",
    "\n",
    "        # Separate the data into bins (which act as pseudo-classes for stratification)\n",
    "        for bin_value in ds.sorted_data['bins'].unique():\n",
    "            bin_data = ds.sorted_data[ds.sorted_data['bins'] == bin_value]\n",
    "\n",
    "            # Shuffle the data\n",
    "            bin_data = bin_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "            # Extract the specified fraction (10% by default) for the tuning set\n",
    "            num_tuning_samples = int(len(bin_data) * tuning_fraction)\n",
    "\n",
    "            # Append tuning set and remaining data\n",
    "            tuning_data.append(bin_data.iloc[:num_tuning_samples])\n",
    "            remaining_data.append(bin_data.iloc[num_tuning_samples:])\n",
    "\n",
    "        # Concatenate tuning and remaining data\n",
    "        ds.tuning_data = pd.concat(tuning_data).reset_index(drop=True)\n",
    "        ds.sorted_data = pd.concat(remaining_data).reset_index(drop=True)\n",
    "\n",
    "        # Drop the 'bins' column, as it's not needed anymore\n",
    "        ds.sorted_data.drop(columns=['bins'], inplace=True)\n",
    "        ds.tuning_data.drop(columns=['bins'], inplace=True)\n",
    "\n",
    "        # Check lengths\n",
    "        #print('Length of tuning data:', len(ds.tuning_data))\n",
    "        #print('Length of remaining data:', len(ds.sorted_data))\n",
    "        #print('Length of original data:', len(ds.sorted_data_bk))    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea427f34",
   "metadata": {},
   "source": [
    "# Demonstrate each of the main operations for the DE: crossover and mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "733e8bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DifferentialEvolution:\n",
    "    \n",
    "    def __init__(self, population_size, mutation_factor, crossover_rate, generations):\n",
    "        self.population_size = population_size\n",
    "        self.mutation_factor = mutation_factor\n",
    "        self.crossover_rate = crossover_rate\n",
    "        self.generations = generations\n",
    "        self.population = None\n",
    "\n",
    "    def initialize_population(self, weight_dims, bounds=(-0.5, 0.5)):\n",
    "        total_dim = sum((in_size * out_size) + out_size for in_size, out_size in weight_dims)\n",
    "        self.population = np.random.uniform(bounds[0], bounds[1], (self.population_size, total_dim))\n",
    "\n",
    "    \n",
    "    \n",
    "    def adjust_weights(self, nn, fitness_scores,x_data, y_data ):\n",
    "                \n",
    "        new_population = np.copy(self.population)\n",
    "\n",
    "        for i in range(self.population_size):\n",
    "            \n",
    "            indices = np.random.choice(\n",
    "                [idx for idx in range(self.population_size) if idx != i], 3, replace=False\n",
    "            )\n",
    "            x1, x2, x3 = self.population[indices]\n",
    "            mutant = x1 + self.mutation_factor * (x2 - x3)\n",
    "\n",
    "            # Crossover\n",
    "            crossover_mask = np.random.rand(mutant.size) < self.crossover_rate\n",
    "            trial = np.where(crossover_mask, mutant, self.population[i])\n",
    "\n",
    "            nn.set_weights(trial)  \n",
    "            trial_fitness = nn.calculate_loss(x_data, y_data)  \n",
    "\n",
    "            # Selection\n",
    "            if trial_fitness < fitness_scores[i]:\n",
    "                new_population[i] = trial\n",
    "                fitness_scores[i] = trial_fitness\n",
    "\n",
    "        self.population = new_population\n",
    "        return new_population[np.argmin(fitness_scores)]  \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "99fe555a",
   "metadata": {},
   "source": [
    "# Layers Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebdc8a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Layer(ABC): \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward_propagation(self, inputData):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1b344ae2",
   "metadata": {},
   "source": [
    "# FC Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b027f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCHiddenLayer(Layer):\n",
    "    \n",
    "    def __init__(self, weights, bias, in_size, out_size):\n",
    "        self.input_size = in_size\n",
    "        self.output_size = out_size\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "        \n",
    "    def forward_propagation(self, inputData):\n",
    "        self.input = inputData  \n",
    "        outputData = np.dot(self.input, self.weights) + self.bias  \n",
    "        self.output = outputData\n",
    "        return self.output\n",
    "    \n",
    "    \n",
    "    def update_weights(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "acc5d571",
   "metadata": {},
   "source": [
    "# Activation Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7254c3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationLayer(Layer):\n",
    "    \n",
    "    def __init__(self, activation):\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward_propagation(self, inputData):\n",
    "        self.input = inputData  \n",
    "        self.output = self.activation(self.input)  \n",
    "        return self.output\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "            \n",
    "    @staticmethod\n",
    "    def linear(x):\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        exp_vals = np.exp(x - np.max(x))  \n",
    "        return exp_vals / np.sum(exp_vals)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8dabb64a",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0ce71c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, lossFunction=None):\n",
    "        self.layers = []\n",
    "        self.loss = lossFunction\n",
    "        \n",
    "    def reset(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def addLayers(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def set_weights(self, individual):\n",
    "        idx = 0\n",
    "       \n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, FCHiddenLayer):\n",
    "                \n",
    "                in_size, out_size = layer.input_size, layer.output_size\n",
    "                weights, bias, idx = self.extract_weights_and_biases(individual, idx, in_size, out_size)\n",
    "                layer.update_weights(weights, bias)\n",
    "\n",
    "                \n",
    "    def calculate_loss(self, train_features, train_class):\n",
    "        return self.train(train_features, train_class)\n",
    "\n",
    "    \n",
    "    \n",
    "    def extract_weights_and_biases(self,individual, idx, in_size, out_size):\n",
    "        weight_size = in_size * out_size\n",
    "        weights = individual[idx:idx + weight_size].reshape(in_size, out_size)\n",
    "        idx += weight_size\n",
    "        biases = individual[idx:idx + out_size]\n",
    "        idx += out_size\n",
    "        return weights, biases, idx\n",
    "    \n",
    "    \n",
    "    def predict(self, test_features):\n",
    "        rows = len(test_features)\n",
    "        result = []\n",
    "\n",
    "        for i in range(rows):\n",
    "            output = test_features[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            result.append(output)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def train(self, train_features, train_class, batch_size=32):\n",
    "        no_of_samples = len(train_features)\n",
    "        total_loss = 0  \n",
    "\n",
    "        indices = np.arange(no_of_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        x_train = train_features[indices]\n",
    "        y_train = train_class[indices]\n",
    "\n",
    "        for j in range(0, no_of_samples, batch_size):\n",
    "            end = min(j + batch_size, no_of_samples)\n",
    "            x_batch = x_train[j:end]\n",
    "            y_batch = y_train[j:end]\n",
    "\n",
    "            output = x_batch\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "\n",
    "            batch_loss = self.loss(y_batch, output)\n",
    "            total_loss += batch_loss  \n",
    "\n",
    "            error = batch_loss  \n",
    "\n",
    "        avg_loss = total_loss / (no_of_samples / batch_size)\n",
    "        #print(f\"Average Loss over all batches: {avg_loss}\")\n",
    "        return avg_loss\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def mse(actual_value, pred_value):\n",
    "        return np.mean(np.power(actual_value-pred_value, 2))\n",
    "\n",
    "    # for classification (binary)\n",
    "    @staticmethod\n",
    "    def binary_crossentropy(actual_value, pred_value):\n",
    "        # Avoid log(0) by clipping values\n",
    "        pred_value = np.clip(pred_value, 1e-12, 1 - 1e-12)\n",
    "        return -np.mean(actual_value * np.log(pred_value) + (1 - actual_value) * np.log(1 - pred_value))\n",
    "\n",
    "\n",
    "    # for multiclass classification\n",
    "    @staticmethod\n",
    "    def categorical_crossentropy(actual_value, pred_value):\n",
    "        # Avoid log(0) by clipping values\n",
    "        pred_value = np.clip(pred_value, 1e-12, 1 - 1e-12)\n",
    "        return -np.mean(np.sum(actual_value * np.log(pred_value), axis=1))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "08701ce5",
   "metadata": {},
   "source": [
    "#building Classification Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06489d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class ClassificationNet:\n",
    "    def __init__(self):\n",
    "        self.net = None\n",
    "    \n",
    "    \n",
    "    def tune_hyperparameters(self, ds, no_of_layers, input_size, x_train, y_train):\n",
    "        \n",
    "        generations_list = [50, 100, 150, 200, 300]\n",
    "        mutation_factors = [0.1, 0.2, 0.3]\n",
    "        crossover_rates = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "        best_accuracy = float('-inf')\n",
    "        best_params = None\n",
    "        best_weights = None\n",
    "\n",
    "        \n",
    "        for generations in generations_list:\n",
    "            for mutation_factor in mutation_factors:\n",
    "                for crossover_rate in crossover_rates:\n",
    "                    print(f\"Testing with generations={generations}, mutation_factor={mutation_factor}, crossover_rate={crossover_rate}\")\n",
    "\n",
    "                   \n",
    "                    de_instance = DifferentialEvolution(\n",
    "                        population_size=300, \n",
    "                        mutation_factor=mutation_factor, \n",
    "                        crossover_rate=crossover_rate, \n",
    "                        generations=generations\n",
    "                    )\n",
    "\n",
    "                    \n",
    "                    if no_of_layers == 0:\n",
    "                        layer_dims = [(input_size, 1)]\n",
    "                    elif no_of_layers == 1:\n",
    "                        layer_dims = [(input_size, 15), (15, 1)]\n",
    "                    elif no_of_layers == 2:\n",
    "                        layer_dims = [(input_size, 15), (15, 15), (15, 1)]\n",
    "\n",
    "                    \n",
    "                    de_instance.initialize_population(layer_dims)\n",
    "\n",
    "                    for generation in range(de_instance.generations):\n",
    "                        fitness_scores = []\n",
    "                        for individual in de_instance.population:\n",
    "                            idx = 0\n",
    "                            self.net.reset()\n",
    "\n",
    "                            \n",
    "                            for in_size, out_size in layer_dims:\n",
    "                                weights, biases, idx = self.extract_weights_and_biases(individual, idx, in_size, out_size)\n",
    "                                self.net.addLayers(FCHiddenLayer(weights, biases, in_size, out_size))\n",
    "                                activation_func = ActivationLayer.tanh if out_size != 1 else ActivationLayer.linear\n",
    "                                self.net.addLayers(ActivationLayer(activation_func))\n",
    "\n",
    "                            \n",
    "                            error = self.net.train(x_train, y_train)\n",
    "                            fitness_scores.append(error)\n",
    "\n",
    "                        \n",
    "                        best_weights = de_instance.adjust_weights(self.net, fitness_scores, x_train, y_train)\n",
    "\n",
    "                        #print(f\"Generation {generation}, Best Fitness: {min(fitness_scores)}\")\n",
    "\n",
    "                    accuracy = 1 - np.min(fitness_scores)  \n",
    "\n",
    "                   \n",
    "                    if accuracy > best_accuracy:\n",
    "                        best_accuracy = accuracy\n",
    "                        best_params = {'generations': generations, 'mutation_factor': mutation_factor, 'crossover_rate': crossover_rate}\n",
    "                        best_weights = best_weights\n",
    "\n",
    "        # Return the best parameters and weights\n",
    "        print(f\"Best Accuracy: {best_accuracy} with parameters: {best_params}\")\n",
    "        return best_params, best_weights\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # we initialize everything here\n",
    "    def makeNeuralNetwork(self, ds, no_of_layers, input_size, x_train, y_train):\n",
    "        \n",
    "        if ds.name == 'breast-cancer-wisconsin':\n",
    "            self.net = NeuralNetwork(NeuralNetwork.binary_crossentropy)\n",
    "            output_activation = ActivationLayer.sigmoid\n",
    "            output_size = 1\n",
    "        else:\n",
    "            self.net = NeuralNetwork(NeuralNetwork.categorical_crossentropy)\n",
    "            output_activation = ActivationLayer.softmax\n",
    "            output_size = len(ds.classes)\n",
    "        \n",
    "        # DE instance\n",
    "        de_instance = DifferentialEvolution(population_size=250, mutation_factor=0.1, crossover_rate=0.8, generations=200)\n",
    "        \n",
    "        # Define network layer structure\n",
    "        if no_of_layers == 0:\n",
    "            layer_dims = [(input_size, output_size)]\n",
    "        \n",
    "        elif no_of_layers == 1:\n",
    "            layer_dims = [(input_size, 16),(16, output_size)]\n",
    "\n",
    "        elif no_of_layers == 2:\n",
    "            layer_dims = [(input_size, 15),(15, 15),(15, output_size)]\n",
    "        \n",
    "    \n",
    "        \n",
    "        de_instance.initialize_population(layer_dims)\n",
    "\n",
    "        for generation in range(de_instance.generations):\n",
    "            fitness_scores = []\n",
    "            for individual in de_instance.population:\n",
    "                idx = 0\n",
    "                self.net.reset()\n",
    "                \n",
    "                # Construct network layers from individual\n",
    "                for in_size, out_size in layer_dims:\n",
    "                    weights, biases, idx = self.extract_weights_and_biases(individual, idx, in_size, out_size)\n",
    "                    self.net.addLayers(FCHiddenLayer(weights, biases, in_size, out_size))\n",
    "                    activation_func = ActivationLayer.tanh if out_size != output_size else output_activation\n",
    "                    self.net.addLayers(ActivationLayer(activation_func))\n",
    "\n",
    "                # Calculate fitness (error)\n",
    "                error = self.net.train(x_train, y_train)\n",
    "                fitness_scores.append(error)\n",
    "\n",
    "            # Update population based on fitness scores\n",
    "            best_weights = de_instance.adjust_weights(self.net, fitness_scores, x_train, y_train )\n",
    "\n",
    "            #print(f\"Generation {generation}, Best Fitness: {min(fitness_scores)}\")\n",
    "\n",
    "        # Return the best weights after all generations\n",
    "        return best_weights\n",
    "\n",
    "    def extract_weights_and_biases(self, individual, idx, in_size, out_size):\n",
    "        weight_size = in_size * out_size\n",
    "        weights = individual[idx:idx + weight_size].reshape(in_size, out_size)\n",
    "        idx += weight_size\n",
    "        biases = individual[idx:idx + out_size]\n",
    "        idx += out_size\n",
    "        return weights, biases, idx\n",
    "    \n",
    "    \n",
    "    \n",
    "    def testFunction(self,ds, no_of_layers):\n",
    "            \n",
    "        ds.test_data = ds.ten_folds[9]\n",
    "        ds.train_data = pd.concat([ds.ten_folds[i] for i in range(9)])\n",
    "\n",
    "        train_features = ds.train_data.iloc[:, :-1]\n",
    "        train_cls = ds.train_data.iloc[:, -1]\n",
    "        test_features = ds.test_data.iloc[:, :-1]\n",
    "        test_cls = ds.test_data.iloc[:, -1]\n",
    "\n",
    "        x_train = train_features.to_numpy()\n",
    "        y_train = train_cls.to_numpy()\n",
    "        x_test = test_features.to_numpy()\n",
    "        y_test = test_cls.to_numpy()\n",
    "        input_size = train_features.shape[1]\n",
    "\n",
    "        # One-Hot Encode the class labels\n",
    "        encoder = OneHotEncoder(sparse_output=False)\n",
    "        y_train = encoder.fit_transform(train_cls.values.reshape(-1, 1))\n",
    "        y_test = encoder.transform(test_cls.values.reshape(-1, 1))\n",
    "\n",
    "        # Train the neural network\n",
    "        best_weights = self.makeNeuralNetwork(ds, no_of_layers, input_size, x_train, y_train)\n",
    "\n",
    "        # Use the best weights to predict on the test set\n",
    "        predictions = self.net.predict(x_test)\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "        # Compute accuracy for this fold\n",
    "        accuracy = np.sum(predicted_classes == true_classes) / len(true_classes)\n",
    "        print(f'Fold 1 Accuracy: {accuracy:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "    def mainFunction(self, ds, no_of_layers):\n",
    "        fold_accuracies = []\n",
    "\n",
    "        for i in range(len(ds.ten_folds)):  # Assuming len(ds.ten_folds) is 10 for 10-fold CV\n",
    "            print(f'Fold: {i}')\n",
    "            ds.test_data = ds.ten_folds[i]\n",
    "            ds.train_data = pd.concat([ds.ten_folds[j] for j in range(len(ds.ten_folds)) if j != i])\n",
    "\n",
    "            train_features = ds.train_data.iloc[:, :-1]\n",
    "            train_cls = ds.train_data.iloc[:, -1]\n",
    "            test_features = ds.test_data.iloc[:, :-1]\n",
    "            test_cls = ds.test_data.iloc[:, -1]\n",
    "\n",
    "            x_train = train_features.to_numpy()\n",
    "            y_train = train_cls.to_numpy()\n",
    "            x_test = test_features.to_numpy()\n",
    "            y_test = test_cls.to_numpy()\n",
    "            input_size = train_features.shape[1]\n",
    "\n",
    "            # One-Hot Encode the class labels\n",
    "            encoder = OneHotEncoder(sparse_output=False)\n",
    "            y_train = encoder.fit_transform(train_cls.values.reshape(-1, 1))\n",
    "            y_test = encoder.transform(test_cls.values.reshape(-1, 1))\n",
    "\n",
    "            # Train the neural network\n",
    "            best_weights = self.makeNeuralNetwork(ds, no_of_layers, input_size, x_train, y_train)\n",
    "\n",
    "            # Use the best weights to predict on the test set\n",
    "            predictions = self.net.predict(x_test)\n",
    "            predicted_classes = np.argmax(predictions, axis=1)\n",
    "            true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "            # Compute accuracy for this fold\n",
    "            accuracy = np.sum(predicted_classes == true_classes) / len(true_classes)\n",
    "            fold_accuracies.append(accuracy)\n",
    "            print(f'Fold {i} Accuracy: {accuracy:.4f}')\n",
    "\n",
    "        # Mean accuracy over all folds\n",
    "        mean_accuracy = np.mean(fold_accuracies)\n",
    "        print(f'Mean Accuracy: {mean_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3040378",
   "metadata": {},
   "source": [
    "#Building regression Neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cdccf3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class RegressionNet:\n",
    "    def __init__(self):\n",
    "        self.net = None\n",
    "    \n",
    "    \n",
    "    # we initialize everything here\n",
    "    def makeNeuralNetwork(self, ds, no_of_layers, input_size, x_train, y_train):\n",
    "        \n",
    "        self.net = NeuralNetwork(NeuralNetwork.mse)\n",
    "        output_activation = ActivationLayer.linear\n",
    "        output_size = 1\n",
    "        \n",
    "        # DE instance\n",
    "        de_instance = DifferentialEvolution(population_size=250, mutation_factor=0.1, crossover_rate=0.8, generations=200)\n",
    "        \n",
    "        # Define network layer structure\n",
    "        if no_of_layers == 0:\n",
    "            layer_dims = [(input_size, output_size)]\n",
    "        \n",
    "        elif no_of_layers == 1:\n",
    "            layer_dims = [(input_size, 17),(17, output_size)]\n",
    "\n",
    "        elif no_of_layers == 2:\n",
    "            layer_dims = [(input_size, 15),(15, 15),(15, output_size)]\n",
    "        \n",
    "    \n",
    "        \n",
    "        de_instance.initialize_population(layer_dims)\n",
    "\n",
    "        for generation in range(de_instance.generations):\n",
    "            fitness_scores = []\n",
    "            for individual in de_instance.population:\n",
    "                idx = 0\n",
    "                self.net.reset()\n",
    "                \n",
    "                # Construct network layers from individual\n",
    "                for in_size, out_size in layer_dims:\n",
    "                    weights, biases, idx = self.extract_weights_and_biases(individual, idx, in_size, out_size)\n",
    "                    self.net.addLayers(FCHiddenLayer(weights, biases, in_size, out_size))\n",
    "                    activation_func = ActivationLayer.tanh if out_size != output_size else output_activation\n",
    "                    self.net.addLayers(ActivationLayer(activation_func))\n",
    "\n",
    "                # Calculate fitness (error)\n",
    "                error = self.net.train(x_train, y_train)\n",
    "                fitness_scores.append(error)\n",
    "\n",
    "            # Update population based on fitness scores\n",
    "            best_weights = de_instance.adjust_weights(self.net, fitness_scores, x_train, y_train )\n",
    "\n",
    "            #print(f\"Generation {generation}, Best Fitness: {min(fitness_scores)}\")\n",
    "\n",
    "        # Return the best weights after all generations\n",
    "        return best_weights\n",
    "\n",
    "    def extract_weights_and_biases(self, individual, idx, in_size, out_size):\n",
    "        weight_size = in_size * out_size\n",
    "        weights = individual[idx:idx + weight_size].reshape(in_size, out_size)\n",
    "        idx += weight_size\n",
    "        biases = individual[idx:idx + out_size]\n",
    "        idx += out_size\n",
    "        return weights, biases, idx\n",
    "\n",
    "    \n",
    "    def testFunction(self,ds, no_of_layers):\n",
    "\n",
    "        \n",
    "        ds.test_data = ds.ten_folds[9]\n",
    "        ds.train_data = pd.concat([ds.ten_folds[i] for i in range(9)])\n",
    "\n",
    "        train_features = ds.train_data.iloc[:, :-1]\n",
    "        train_cls = ds.train_data.iloc[:, -1]\n",
    "        test_features = ds.test_data.iloc[:, :-1]\n",
    "        test_cls = ds.test_data.iloc[:, -1]\n",
    "\n",
    "        x_train = train_features.to_numpy()\n",
    "        y_train = train_cls.to_numpy()\n",
    "        x_test = test_features.to_numpy()\n",
    "        y_test = test_cls.to_numpy()\n",
    "        input_size = train_features.shape[1]\n",
    "\n",
    "        # Train the neural network\n",
    "        best_weights = self.makeNeuralNetwork(ds, no_of_layers, input_size, x_train, y_train)\n",
    "\n",
    "        predictions = self.net.predict(x_test)\n",
    "        results = pd.DataFrame({'Actual Class': y_test, 'Predicted Class': predictions})\n",
    "\n",
    "        mse = self.mse(results['Actual Class'], results['Predicted Class'])\n",
    "        print(f'Fold 1: , Mean Squared Error: {mse}')\n",
    "\n",
    "    \n",
    "    def mainFunction(self, ds, no_of_layers):\n",
    "        fold_errors = []\n",
    "\n",
    "        for i in range(len(ds.ten_folds)):  # Assuming len(ds.ten_folds) is 10 for 10-fold CV\n",
    "            print(f'Fold: {i}')\n",
    "            ds.test_data = ds.ten_folds[i]\n",
    "            ds.train_data = pd.concat([ds.ten_folds[j] for j in range(len(ds.ten_folds)) if j != i])\n",
    "\n",
    "            train_features = ds.train_data.iloc[:, :-1]\n",
    "            train_cls = ds.train_data.iloc[:, -1]\n",
    "            test_features = ds.test_data.iloc[:, :-1]\n",
    "            test_cls = ds.test_data.iloc[:, -1]\n",
    "\n",
    "            x_train = train_features.to_numpy()\n",
    "            y_train = train_cls.to_numpy()\n",
    "            x_test = test_features.to_numpy()\n",
    "            y_test = test_cls.to_numpy()\n",
    "            input_size = train_features.shape[1]\n",
    "\n",
    "            # Train the neural network\n",
    "            best_weights = self.makeNeuralNetwork(ds, no_of_layers, input_size, x_train, y_train)\n",
    "\n",
    "            predictions = self.net.predict(x_test)\n",
    "            results = pd.DataFrame({'Actual Class': y_test, 'Predicted Class': predictions})\n",
    "\n",
    "            mse = self.mse(results['Actual Class'], results['Predicted Class'])\n",
    "            print(f'Fold: {i}, Mean Squared Error: {mse}')\n",
    "\n",
    "            fold_errors.append(mse)\n",
    "\n",
    "            print(f'Fold: {i}, Error: {mse}')\n",
    "\n",
    "        # After all folds, calculate and print the mean error\n",
    "        mean_errors = np.mean(fold_errors)\n",
    "        print(f'Mean Error across all folds: {mean_errors}')\n",
    "        \n",
    "    def mse(self, actual_value, pred_value):\n",
    "        return np.mean(np.power(actual_value - pred_value, 2))   \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "61686a9b",
   "metadata": {},
   "source": [
    "# Main Driver Class\n",
    "# Provide sample outputs from one test fold showing performance on one classification and one regression\n",
    "network. Show results for the two hidden layer cases only but for each of the learning methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be0898a5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Main driver class calls all other classes  \n",
    "\n",
    "class Driver:\n",
    "    def __init__(self):\n",
    "        self.DSNames_classification = 'glass', 'soybean-small', 'breast-cancer-wisconsin'\n",
    "        self.DSNames_regression = 'abalone', 'machine', 'forestfires'\n",
    "    \n",
    "    #classification\n",
    "    def main_classification(self):\n",
    "        \n",
    "        #Preprocessing part\n",
    "        #change the dataset number here from 0 to 2\n",
    "        ds = DataSet(self.DSNames_classification[0])\n",
    "        ds.type = 'Classification'\n",
    "        ds_reader = Dataset_Reader()\n",
    "        ds_reader.read_data_file(ds)\n",
    "            \n",
    "        pp = PreProcess(ds)\n",
    "        pp.split_tuning_data_classification(ds)\n",
    "        pp.split_data_classification(ds)\n",
    "        \n",
    "        cn = ClassificationNet()\n",
    "        #print('Neural net with 0 hidden layers')\n",
    "        #cn.mainFunction(ds, 0)\n",
    "        #print('Neural net with 1 hidden layers')\n",
    "        #cn.mainFunction(ds, 1)\n",
    "        print('\\nJust one fold result with 2 hidden layers for DE')\n",
    "        cn.testFunction(ds,2)\n",
    "       \n",
    "        \n",
    "    #Regression\n",
    "    def main_regression(self):\n",
    "        #change the dataset number here from 0 to 2\n",
    "        ds = DataSet(self.DSNames_regression[1])\n",
    "        ds.type = 'Regression'\n",
    "        ds_reader = Dataset_Reader()\n",
    "        ds_reader.read_data_file(ds)\n",
    "        \n",
    "        pp = PreProcess(ds)\n",
    "        pp.split_tuning_data_regression(ds)\n",
    "        pp.split_data_regression(ds)\n",
    "        \n",
    "        rn = RegressionNet()\n",
    "        #print('Neural net with 0 hidden layers')\n",
    "        #rn.mainFunction(ds, 0)\n",
    "        #print('Neural net with 1 hidden layers')\n",
    "        #rn.mainFunction(ds, 1)\n",
    "        print('\\nJust one fold result with 2 hidden layers for DE')\n",
    "        rn.testFunction(ds,2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe87b21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Net\n",
      "\n",
      "Just one fold result with 2 hidden layers for DE\n",
      "Fold 1 Accuracy: 0.5625\n",
      "\n",
      "Just one fold result with 2 hidden layers for PSO\n",
      "Fold 1 Accuracy: 0.8750\n",
      "\n",
      "Just one fold result with 2 hidden layers for GA\n",
      "Fold 1 Accuracy: 0.6875\n"
     ]
    }
   ],
   "source": [
    "#calling the driver class here.    \n",
    "dr = Driver()\n",
    "print('Classification Net')\n",
    "dr.main_classification() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0774b1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Net\n",
      "\n",
      "Just one fold result with 2 hidden layers for DE\n",
      "Fold 1: , Mean Squared Error: [0.18000208]\n",
      "\n",
      "Just one fold result with 2 hidden layers for PSO\n",
      "Fold 1: , Mean Squared Error: [0.17140149]\n",
      "\n",
      "Just one fold result with 2 hidden layers for GA\n",
      "Fold 1: , Mean Squared Error: [0.18240524]\n"
     ]
    }
   ],
   "source": [
    "print('Regression Net')\n",
    "dr.main_regression()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7b68c636",
   "metadata": {},
   "source": [
    "# Show the average performance over the ten folds for one of the classification data sets and one of the\n",
    "regression data sets for each of the networks trained with each of the algorithms. (Note that results\n",
    "for all networks and all data sets are required in your paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70b58cc8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Classification Net for Glass Dataset\n",
      "\n",
      " Neural net with 0 hidden layers, Average Accuracy\n",
      "Fold: 0\n",
      "Fold 0 Accuracy: 0.6364\n",
      "Fold: 1\n",
      "Fold 1 Accuracy: 0.6818\n",
      "Fold: 2\n",
      "Fold 2 Accuracy: 0.6190\n",
      "Fold: 3\n",
      "Fold 3 Accuracy: 0.5500\n",
      "Fold: 4\n",
      "Fold 4 Accuracy: 0.6000\n",
      "Fold: 5\n",
      "Fold 5 Accuracy: 0.6500\n",
      "Fold: 6\n",
      "Fold 6 Accuracy: 0.6316\n",
      "Fold: 7\n",
      "Fold 7 Accuracy: 0.6667\n",
      "Fold: 8\n",
      "Fold 8 Accuracy: 0.6471\n",
      "Fold: 9\n",
      "Fold 9 Accuracy: 0.5625\n",
      "Mean Accuracy: 0.6245\n",
      "\n",
      " Neural net with 1 hidden layers, Average Accuracy\n",
      "Fold: 0\n",
      "Fold 0 Accuracy: 0.7727\n",
      "Fold: 1\n",
      "Fold 1 Accuracy: 0.7727\n",
      "Fold: 2\n",
      "Fold 2 Accuracy: 0.7143\n",
      "Fold: 3\n",
      "Fold 3 Accuracy: 0.6500\n",
      "Fold: 4\n",
      "Fold 4 Accuracy: 0.8000\n",
      "Fold: 5\n",
      "Fold 5 Accuracy: 0.7500\n",
      "Fold: 6\n",
      "Fold 6 Accuracy: 0.6842\n",
      "Fold: 7\n",
      "Fold 7 Accuracy: 0.7222\n",
      "Fold: 8\n",
      "Fold 8 Accuracy: 0.8824\n",
      "Fold: 9\n",
      "Fold 9 Accuracy: 0.6875\n",
      "Mean Accuracy: 0.7436\n",
      "\n",
      " Neural net with 2 hidden layers, Average Accuracy\n",
      "Fold: 0\n",
      "Fold 0 Accuracy: 0.5909\n",
      "Fold: 1\n",
      "Fold 1 Accuracy: 0.7273\n",
      "Fold: 2\n",
      "Fold 2 Accuracy: 0.1429\n",
      "Fold: 3\n",
      "Fold 3 Accuracy: 0.5000\n",
      "Fold: 4\n",
      "Fold 4 Accuracy: 0.7500\n",
      "Fold: 5\n",
      "Fold 5 Accuracy: 0.6500\n",
      "Fold: 6\n",
      "Fold 6 Accuracy: 0.8421\n",
      "Fold: 7\n",
      "Fold 7 Accuracy: 0.5556\n",
      "Fold: 8\n",
      "Fold 8 Accuracy: 0.8824\n",
      "Fold: 9\n",
      "Fold 9 Accuracy: 0.7500\n",
      "Mean Accuracy: 0.6391\n",
      "\n",
      " Regression Net for Machine Dataset\n",
      "\n",
      " Neural net with 0 hidden layers, Anerage MSE\n",
      "Fold: 0\n",
      "Fold: 0, Mean Squared Error: [0.01177329]\n",
      "Fold: 0, Error: [0.01177329]\n",
      "Fold: 1\n",
      "Fold: 1, Mean Squared Error: [0.00660694]\n",
      "Fold: 1, Error: [0.00660694]\n",
      "Fold: 2\n",
      "Fold: 2, Mean Squared Error: [0.00947169]\n",
      "Fold: 2, Error: [0.00947169]\n",
      "Fold: 3\n",
      "Fold: 3, Mean Squared Error: [0.00860316]\n",
      "Fold: 3, Error: [0.00860316]\n",
      "Fold: 4\n",
      "Fold: 4, Mean Squared Error: [0.00229302]\n",
      "Fold: 4, Error: [0.00229302]\n",
      "Fold: 5\n",
      "Fold: 5, Mean Squared Error: [0.00711991]\n",
      "Fold: 5, Error: [0.00711991]\n",
      "Fold: 6\n",
      "Fold: 6, Mean Squared Error: [0.00087321]\n",
      "Fold: 6, Error: [0.00087321]\n",
      "Fold: 7\n",
      "Fold: 7, Mean Squared Error: [0.02271741]\n",
      "Fold: 7, Error: [0.02271741]\n",
      "Fold: 8\n",
      "Fold: 8, Mean Squared Error: [0.00545978]\n",
      "Fold: 8, Error: [0.00545978]\n",
      "Fold: 9\n",
      "Fold: 9, Mean Squared Error: [0.15384726]\n",
      "Fold: 9, Error: [0.15384726]\n",
      "Mean Error across all folds: 0.022876565840233152\n",
      "\n",
      " Neural net with 1 hidden layers, Average MSE\n",
      "Fold: 0\n",
      "Fold: 0, Mean Squared Error: [0.01116228]\n",
      "Fold: 0, Error: [0.01116228]\n",
      "Fold: 1\n",
      "Fold: 1, Mean Squared Error: [0.01424501]\n",
      "Fold: 1, Error: [0.01424501]\n",
      "Fold: 2\n",
      "Fold: 2, Mean Squared Error: [0.00992744]\n",
      "Fold: 2, Error: [0.00992744]\n",
      "Fold: 3\n",
      "Fold: 3, Mean Squared Error: [0.00327916]\n",
      "Fold: 3, Error: [0.00327916]\n",
      "Fold: 4\n",
      "Fold: 4, Mean Squared Error: [0.00386054]\n",
      "Fold: 4, Error: [0.00386054]\n",
      "Fold: 5\n",
      "Fold: 5, Mean Squared Error: [0.00067345]\n",
      "Fold: 5, Error: [0.00067345]\n",
      "Fold: 6\n",
      "Fold: 6, Mean Squared Error: [0.00041649]\n",
      "Fold: 6, Error: [0.00041649]\n",
      "Fold: 7\n",
      "Fold: 7, Mean Squared Error: [0.00152043]\n",
      "Fold: 7, Error: [0.00152043]\n",
      "Fold: 8\n",
      "Fold: 8, Mean Squared Error: [0.00427443]\n",
      "Fold: 8, Error: [0.00427443]\n",
      "Fold: 9\n",
      "Fold: 9, Mean Squared Error: [0.17412225]\n",
      "Fold: 9, Error: [0.17412225]\n",
      "Mean Error across all folds: 0.022348148033963358\n",
      "\n",
      " Neural net with 2 hidden layers, Average MSE\n",
      "Fold: 0\n",
      "Fold: 0, Mean Squared Error: [0.00539315]\n",
      "Fold: 0, Error: [0.00539315]\n",
      "Fold: 1\n",
      "Fold: 1, Mean Squared Error: [0.01012356]\n",
      "Fold: 1, Error: [0.01012356]\n",
      "Fold: 2\n",
      "Fold: 2, Mean Squared Error: [0.00187346]\n",
      "Fold: 2, Error: [0.00187346]\n",
      "Fold: 3\n",
      "Fold: 3, Mean Squared Error: [0.0055577]\n",
      "Fold: 3, Error: [0.0055577]\n",
      "Fold: 4\n",
      "Fold: 4, Mean Squared Error: [0.00087708]\n",
      "Fold: 4, Error: [0.00087708]\n",
      "Fold: 5\n",
      "Fold: 5, Mean Squared Error: [0.00423578]\n",
      "Fold: 5, Error: [0.00423578]\n",
      "Fold: 6\n",
      "Fold: 6, Mean Squared Error: [0.00019155]\n",
      "Fold: 6, Error: [0.00019155]\n",
      "Fold: 7\n",
      "Fold: 7, Mean Squared Error: [0.00088976]\n",
      "Fold: 7, Error: [0.00088976]\n",
      "Fold: 8\n",
      "Fold: 8, Mean Squared Error: [0.00255023]\n",
      "Fold: 8, Error: [0.00255023]\n",
      "Fold: 9\n",
      "Fold: 9, Mean Squared Error: [0.17741208]\n",
      "Fold: 9, Error: [0.17741208]\n",
      "Mean Error across all folds: 0.020910433902747664\n"
     ]
    }
   ],
   "source": [
    "#Main driver class calls all other classes  \n",
    "\n",
    "class FullDriverClass:\n",
    "    def __init__(self):\n",
    "        self.DSNames_classification = 'glass', 'soybean-small', 'breast-cancer-wisconsin'\n",
    "        self.DSNames_regression = 'abalone', 'machine', 'forestfires'\n",
    "\n",
    "\n",
    "#classification\n",
    "    def main_classification(self):\n",
    "        \n",
    "        #Preprocessing part\n",
    "        #change the dataset number here from 0 to 2\n",
    "        ds = DataSet(self.DSNames_classification[0])\n",
    "        ds.type = 'Classification'\n",
    "        ds_reader = Dataset_Reader()\n",
    "        ds_reader.read_data_file(ds)\n",
    "            \n",
    "        pp = PreProcess(ds)\n",
    "        pp.split_tuning_data_classification(ds)\n",
    "        pp.split_data_classification(ds)\n",
    "        \n",
    "        cn = ClassificationNet()\n",
    "        print('\\n Neural net with 0 hidden layers, Average Accuracy')\n",
    "        cn.mainFunction(ds, 0)\n",
    "        print('\\n Neural net with 1 hidden layers, Average Accuracy')\n",
    "        cn.mainFunction(ds, 1)\n",
    "        print('\\n Neural net with 2 hidden layers, Average Accuracy')\n",
    "        cn.mainFunction(ds, 2)\n",
    "        \n",
    "       \n",
    "        \n",
    "    #Regression\n",
    "    def main_regression(self):\n",
    "        #change the dataset number here from 0 to 2\n",
    "        ds = DataSet(self.DSNames_regression[1])\n",
    "        ds.type = 'Regression'\n",
    "        ds_reader = Dataset_Reader()\n",
    "        ds_reader.read_data_file(ds)\n",
    "        \n",
    "        pp = PreProcess(ds)\n",
    "        pp.split_tuning_data_regression(ds)\n",
    "        pp.split_data_regression(ds)\n",
    "        \n",
    "        rn = RegressionNet()\n",
    "        print('\\n Neural net with 0 hidden layers, Anerage MSE')\n",
    "        rn.mainFunction(ds, 0)\n",
    "        print('\\n Neural net with 1 hidden layers, Average MSE')\n",
    "        rn.mainFunction(ds, 1)\n",
    "        print('\\n Neural net with 2 hidden layers, Average MSE')\n",
    "        rn.mainFunction(ds, 2)\n",
    "\n",
    "        \n",
    "fdr = FullDriverClass()\n",
    "print('\\n Classification Net for Glass Dataset')\n",
    "fdr.main_classification() \n",
    "print('\\n Regression Net for Machine Dataset')\n",
    "fdr.main_regression()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51efe43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
